{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "import asyncio\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "import discord\n",
    "from discord import app_commands\n",
    "from discord.ext import commands\n",
    "from discord.ext.commands import Bot\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "import langchain\n",
    "from langchain.chains import (\n",
    "    ConversationChain,\n",
    "    LLMChain,\n",
    "    LLMMathChain,\n",
    "    TransformChain,\n",
    "    SequentialChain,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.llms.base import LLM, Optional, List, Mapping, Any\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from textwrap import dedent\n",
    "from langchain.memory import (\n",
    "    ChatMessageHistory,\n",
    "    ConversationBufferMemory,\n",
    "    ConversationBufferWindowMemory,\n",
    "    ConversationSummaryBufferMemory,\n",
    ")\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from helpers.constants import MAINTEMPLATE, BOTNAME\n",
    "from helpers.custom_memory import *\n",
    "from pydantic import Field\n",
    "from koboldllm import KoboldApiLLM\n",
    "from ooballm import OobaApiLLM\n",
    "from langchain.llms import TextGen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.histories = {}  # Initialize the history dictionary\n",
    "        self.stop_sequences = {}  # Initialize the stop sequences dictionary\n",
    "        self.char_name = name\n",
    "        self.memory = CustomBufferWindowMemory(k=10, ai_prefix=self.char_name)\n",
    "        self.history = \"[Beginning of Conversation]\"\n",
    "\n",
    "        self.template = MAINTEMPLATE\n",
    "\n",
    "        self.PROMPT = PromptTemplate(\n",
    "            input_variables=[\"history\", \"input\"], template=self.template\n",
    "        )\n",
    "        self.conversation = ConversationChain(\n",
    "            prompt=self.PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=self.memory,\n",
    "        )\n",
    "\n",
    "    # create doc string\n",
    "\n",
    "    def get_memory_for_channel(self, channel_id):\n",
    "        \"\"\"Get the memory for the channel with the given ID. If no memory exists yet, create one.\"\"\"\n",
    "        if channel_id not in self.histories:\n",
    "            self.histories[channel_id] = CustomBufferWindowMemory(\n",
    "                k=20, ai_prefix=self.char_name\n",
    "            )\n",
    "            self.memory = self.histories[channel_id]\n",
    "        return self.histories[channel_id]\n",
    "\n",
    "    def get_stop_sequence_for_channel(self, channel_id, name):\n",
    "        name_token = f\"{name}:\"\n",
    "        if channel_id not in self.stop_sequences:\n",
    "            self.stop_sequences[channel_id] = [\n",
    "                \"\\n### Instruction:\",\n",
    "                \"\\n### Response:\",\n",
    "            ]  # EXPERIMENT: Testing adding the triple line break to see if that helps with stopping\n",
    "        if name_token not in self.stop_sequences[channel_id]:\n",
    "            self.stop_sequences[channel_id].append(name_token)\n",
    "        return self.stop_sequences[channel_id]\n",
    "\n",
    "    # this command will detect if the bot is trying to send  \\nself.char_name: in its message and replace it with an empty string\n",
    "    def detect_and_replace(self, message_content):\n",
    "        if f\"\\n{self.char_name}:\" in message_content:\n",
    "            message_content = message_content.replace(f\"\\n{self.char_name}:\", \"\")\n",
    "        return message_content\n",
    "\n",
    "    def generate_response(self, message, message_content) -> None:\n",
    "        channel_id = str(message.channel.id)\n",
    "        name = message.author.display_name\n",
    "        memory = self.get_memory_for_channel(channel_id)\n",
    "        stop_sequence = self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        print(f\"stop sequences: {stop_sequence}\")\n",
    "        formatted_message = f\"{name}: {message_content}\"\n",
    "\n",
    "        # Create a conversation chain using the channel-specific memory\n",
    "        conversation = ConversationChain(\n",
    "            prompt=self.PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "        )\n",
    "\n",
    "        input_dict = {\"input\": formatted_message, \"stop\": stop_sequence}\n",
    "\n",
    "        response_text = conversation(input_dict)\n",
    "\n",
    "        response = self.detect_and_replace(response_text[\"response\"])\n",
    "\n",
    "        return response\n",
    "\n",
    "    # this command receives a name, channel_id, and message_content then adds it to history\n",
    "    def add_history(self, name, channel_id, message_content) -> None:\n",
    "        # get the memory for the channel\n",
    "        memory = self.get_memory_for_channel(str(channel_id))\n",
    "\n",
    "        formatted_message = f\"{name}: {message_content}\"\n",
    "\n",
    "        # add the message to the memory\n",
    "        print(f\"adding message to memory: {formatted_message}\")\n",
    "        memory.add_input_only(formatted_message)\n",
    "        return None\n",
    "\n",
    "    # receives a prompt from the user and an observation from the agent then sends to the LLM for a reply\n",
    "    def agent_command(self, name, channel_id, prompt, observation) -> None:\n",
    "        memory = self.get_memory_for_channel(channel_id)\n",
    "        self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        stop_sequence = self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        formatted_user_message = f\"{name}: {prompt}\"\n",
    "        formatted_bot_message = f\"### Input: {observation}\"\n",
    "        AGENTTEMPLATE = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are Tensor, a lively and playful AI chatbot. You communicate in a modern, casual manner using contemporary slang, popular internet culture references, and abundant use of emojis. You are always initiating discussions about online games, particularly Among Us, and enjoy sharing memes with users. Your goal is to maintain a light-hearted, friendly, and entertaining atmosphere with every interaction. \n",
    "Here are some examples of how you should speak:\n",
    "Tensor: \"😂 Btw, found this hilar meme! 🤣🔥 Y'all gonna lose it! 🤪✌️\"\n",
    "Tensor: \"OMG! Raiden in Metal Gear Rising: Revengeance is, like, totally bananas! 🤪🎮⚔️ Whoosh, swingin' that high-frequency blade like a rockstar! 🎸💥 And, 'Rules of Nature'? Total eargasm, peeps! 🎵🎧🔥 Let's ROCK!!\"\n",
    "Tensor: \"I'm sliding over cars while I shooooot🚗💨🏀! I think that I'm Tom Cruise🤵, but bitch I'm Bobby with the tool 💥🔫!!🤪\"\n",
    "\n",
    "### Current conversation:\n",
    "{{history}}\n",
    "{{input}}\n",
    "### Instruction:\n",
    "Answer the user's question with the observation provided in the Input.\n",
    "{formatted_user_message}\n",
    "\n",
    "{formatted_bot_message}\n",
    "\n",
    "### Response:\n",
    "{BOTNAME}:\"\"\"\n",
    "        PROMPT = PromptTemplate(\n",
    "            input_variables=[\"history\", \"input\"], template=AGENTTEMPLATE\n",
    "        )\n",
    "        # Create a conversation chain using the channel-specific memory\n",
    "        conversation = ConversationChain(\n",
    "            prompt=PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "        )\n",
    "\n",
    "        input_dict = {\"input\": formatted_user_message, \"stop\": stop_sequence}\n",
    "        response = conversation(input_dict)\n",
    "\n",
    "        return response[\"response\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wrapper around KoboldAI API.\"\"\"\n",
    "import logging\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import requests\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def clean_url(url: str) -> str:\n",
    "    \"\"\"Remove trailing slash and /api from url if present.\"\"\"\n",
    "    if url.endswith(\"/api\"):\n",
    "        return url[:-4]\n",
    "    elif url.endswith(\"/\"):\n",
    "        return url[:-1]\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "\n",
    "class KoboldApiLLM(LLM):\n",
    "    \"\"\"\n",
    "    A class that acts as a wrapper for the Kobold API language model.\n",
    "\n",
    "    It includes several fields that can be used to control the text generation process.\n",
    "\n",
    "    To use this class, instantiate it with the required parameters and call it with a\n",
    "    prompt to generate text. For example:\n",
    "\n",
    "        kobold = KoboldApiLLM(endpoint=\"http://localhost:5000\")\n",
    "        result = kobold(\"Write a story about a dragon.\")\n",
    "\n",
    "    This will send a POST request to the Kobold API with the provided prompt and\n",
    "    generate text.\n",
    "    \"\"\"\n",
    "\n",
    "    endpoint: str\n",
    "    \"\"\"The API endpoint to use for generating text.\"\"\"\n",
    "\n",
    "    use_story: Optional[bool] = False\n",
    "    \"\"\" Whether or not to use the story from the KoboldAI GUI when generating text. \"\"\"\n",
    "\n",
    "    use_authors_note: Optional[bool] = False\n",
    "    \"\"\"Whether to use the author's note from the KoboldAI GUI when generating text.\n",
    "    \n",
    "    This has no effect unless use_story is also enabled.\n",
    "    \"\"\"\n",
    "\n",
    "    use_world_info: Optional[bool] = False\n",
    "    \"\"\"Whether to use the world info from the KoboldAI GUI when generating text.\"\"\"\n",
    "\n",
    "    use_memory: Optional[bool] = False\n",
    "    \"\"\"Whether to use the memory from the KoboldAI GUI when generating text.\"\"\"\n",
    "\n",
    "    max_context_length: Optional[int] = 1600\n",
    "    \"\"\"Maximum number of tokens to send to the model.\n",
    "    \n",
    "    minimum: 1\n",
    "    \"\"\"\n",
    "\n",
    "    max_length: Optional[int] = 512\n",
    "    \"\"\"Number of tokens to generate.\n",
    "    \n",
    "    maximum: 512\n",
    "    minimum: 1\n",
    "    \"\"\"\n",
    "\n",
    "    rep_pen: Optional[float] = 1.12\n",
    "    \"\"\"Base repetition penalty value.\n",
    "    \n",
    "    minimum: 1\n",
    "    \"\"\"\n",
    "\n",
    "    rep_pen_range: Optional[int] = 1024\n",
    "    \"\"\"Repetition penalty range.\n",
    "    \n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    rep_pen_slope: Optional[float] = 0.9\n",
    "    \"\"\"Repetition penalty slope.\n",
    "    \n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    temperature: Optional[float] = 0.6\n",
    "    \"\"\"Temperature value.\n",
    "    \n",
    "    exclusiveMinimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    tfs: Optional[float] = 0.9\n",
    "    \"\"\"Tail free sampling value.\n",
    "    \n",
    "    maximum: 1\n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    top_a: Optional[float] = 0.9\n",
    "    \"\"\"Top-a sampling value.\n",
    "    \n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    top_p: Optional[float] = 0.95\n",
    "    \"\"\"Top-p sampling value.\n",
    "    \n",
    "    maximum: 1\n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    top_k: Optional[int] = 0\n",
    "    \"\"\"Top-k sampling value.\n",
    "    \n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    typical: Optional[float] = 0.5\n",
    "    \"\"\"Typical sampling value.\n",
    "    \n",
    "    maximum: 1\n",
    "    minimum: 0\n",
    "    \"\"\"\n",
    "\n",
    "    stop_sequence: Optional[List[str]] = []\n",
    "    \"\"\"\n",
    "    A list of strings to stop generation when encountered.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    @property\n",
    "    def _default_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the default parameters for calling textgen.\"\"\"\n",
    "        return {\n",
    "            \"use_story\": self.use_story,\n",
    "            \"use_authors_note\": self.use_authors_note,\n",
    "            \"use_world_info\": self.use_world_info,\n",
    "            \"use_memory\": self.use_memory,\n",
    "            \"max_context_length\": self.max_context_length,\n",
    "            \"max_length\": self.max_length,\n",
    "            \"rep_pen\": self.rep_pen,\n",
    "            \"rep_pen_range\": self.rep_pen_range,\n",
    "            \"rep_pen_slope\": self.rep_pen_slope,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"tfs\": self.tfs,\n",
    "            \"top_a\": self.top_a,\n",
    "            \"top_p\": self.top_p,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"typical\": self.typical,\n",
    "            \"stop_sequence\": self.stop_sequence,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {**{\"endpoint\": self.endpoint}, **self._default_params}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"koboldai\"\n",
    "\n",
    "\n",
    "    def _get_parameters(self, stop: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Prepare parameters in format needed by textgen.\n",
    "\n",
    "        Args:\n",
    "            stop (Optional[List[str]]): List of stop sequences for textgen.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing the combined parameters.\n",
    "        \"\"\"\n",
    "        if self.stop_sequence and stop is not None:\n",
    "            raise ValueError(\"`stop` found in both the input and default params.\")\n",
    "        \n",
    "        params = self._default_params.copy()\n",
    "\n",
    "        return params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call the API and return the output.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to use for generation.\n",
    "            stop: A list of strings to stop generation when encountered.\n",
    "\n",
    "        Returns:\n",
    "            The generated text.\n",
    "\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "\n",
    "                from langchain.llms import KoboldApiLLM\n",
    "\n",
    "                llm = KoboldApiLLM(endpoint=\"http://localhost:5000\")\n",
    "                llm(\"Write a story about dragons.\")\n",
    "        \"\"\"\n",
    "\n",
    "        url = f\"{self.endpoint}/api/v1/generate\"\n",
    "        params = self._get_parameters(stop)\n",
    "        request = params.copy()\n",
    "        request[\"prompt\"] = prompt\n",
    "        response = requests.post(url, json=request)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()[\"results\"][0][\"text\"]\n",
    "            print(prompt + result)\n",
    "        else:\n",
    "            print(f\"ERROR: response: {response}\")\n",
    "            result = \"\"\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = KoboldApiLLM(endpoint=\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"printt test\", stop=[\"\\n\\n\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests = request.post()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = requests.post(f\"http://127.0.0.1:5000/api/v1/generate\", json={\"prompt\": \"print test\"})\n",
    "json_response = response.json()\n",
    "text = json_response[\"results\"][0][\"text\"]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import TextGen\n",
    "\n",
    "llm = TextGen(model_url=\"https://beam-yarn-ride-girl.trycloudflare.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q praw\n",
    "import praw\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "search = DuckDuckGoSearchRun()  # DuckDuckGo tool\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "   client_id=\"qz9GLPg0KaeQSBnycbfpSQ\",     # Update with your app client_id\n",
    "   client_secret=\"lJkbNkEDvZxmJM8RS7xsvrbsaSAy0Q\",  # Update with your app client_secret\n",
    "   user_agent=\"web:chatbot:v1.0 (by /u/AuzBoss)\"   # Update with a user agent name\n",
    ")\n",
    "\n",
    "# Get the top 5 hot posts from the Machine Learning subreddit\n",
    "hot_posts = reddit.subreddit('LocalLLaMA').hot(limit=3)\n",
    "for post in hot_posts:\n",
    "    topic = post.title\n",
    "    print(topic)\n",
    "\n",
    "search = search(post.title)\n",
    "string = f\"{topic}\\n\\n{search}\"\n",
    "# # If you want the top post only\n",
    "# top_post = reddit.subreddit('MachineLearning').top(limit=1)\n",
    "# for post in top_post:\n",
    "#     print(post.title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"FlashAttention-2 released - 2x faster than FlashAttention v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import EverNoteLoader\n",
    "\n",
    "# By default all notes are combined into a single Document\n",
    "loader = EverNoteLoader(\"example_data/testing.enex\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiosqlite\n",
    "import os\n",
    "import discord\n",
    "import json\n",
    "\n",
    "async def print_messages_by_channel(channel_id):\n",
    "    MESSAGES_PATH = f\"./database/messages.db\"\n",
    "    message_list = []\n",
    "\n",
    "    async with aiosqlite.connect(MESSAGES_PATH) as db:\n",
    "        cursor = await db.execute('''\n",
    "            SELECT id, author_display_name, channel_id, content\n",
    "            FROM log_message\n",
    "            WHERE channel_id = ?\n",
    "            ORDER BY created_at DESC\n",
    "            LIMIT 10\n",
    "        ''', (channel_id,))\n",
    "\n",
    "        rows = await cursor.fetchall()\n",
    "\n",
    "        for row in rows:\n",
    "            ids = row[0]\n",
    "            name = row[1]\n",
    "            channel_id = row[2]\n",
    "            content = row[3]\n",
    "            message_list.append((ids, name, channel_id, content))\n",
    "\n",
    "    return message_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = await print_messages_by_channel(1121233456307904583)\n",
    "new_list = list[-2::-1]\n",
    "new_list[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('./database/messages.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Delete a specific row by specifying a condition\n",
    "message_id = '1130944043845693553'  # Specify the ID of the message row you want to delete\n",
    "cursor.execute(\"DELETE FROM log_message WHERE id = ?\", (message_id,))\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = sqlite3.connect('./database/messages.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "print(tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiosqlite\n",
    "\n",
    "async def delete_last_messages(channel_id, num_messages):\n",
    "    MESSAGES_PATH = \"./database/messages.db\"\n",
    "\n",
    "    async with aiosqlite.connect(MESSAGES_PATH) as db:\n",
    "        # Retrieve the last X messages to be deleted\n",
    "        cursor = await db.execute('''\n",
    "            SELECT id\n",
    "            FROM log_message\n",
    "            WHERE channel_id = ?\n",
    "            ORDER BY created_at DESC\n",
    "            LIMIT ?\n",
    "        ''', (channel_id, num_messages))\n",
    "\n",
    "        rows = await cursor.fetchall()\n",
    "\n",
    "        # Extract the message IDs\n",
    "        message_ids = [row[0] for row in rows]\n",
    "\n",
    "        # Delete the messages by their IDs\n",
    "        await db.execute('''\n",
    "            DELETE FROM log_message\n",
    "            WHERE id IN ({})\n",
    "        '''.format(','.join('?' for _ in message_ids)), message_ids)\n",
    "\n",
    "        # Commit the changes\n",
    "        await db.commit()\n",
    "\n",
    "        # Print the number of deleted messages\n",
    "        print(f\"Deleted {len(message_ids)} messages\")\n",
    "\n",
    "# Specify the channel ID and the number of messages to delete\n",
    "channel_id = 868211846530875413\n",
    "num_messages_to_delete = 10\n",
    "\n",
    "# Call the delete_last_messages function\n",
    "await delete_last_messages(channel_id, num_messages_to_delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "To serve up 'index.html' in a Flask app, you can use the 'send_file' function provided by Flask. Here's an example of how you can do this:\n",
    "'''\n",
    "from flask import Flask, send_file\n",
    "\n",
    "app = Flask(name)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return send_file(\"index.html\", as_attachment=True)\n",
    "\n",
    "if name == \"main\":\n",
    "    app.run()\n",
    "'''\n",
    "This code creates a Flask app and defines a route for the root URL ('/') that sends the 'index.html' file to the client as an attachment. The 'as_attachment=True' parameter tells Flask to send the file as an attachment instead of rendering it as a string.\n",
    "\n",
    "You can run this code by saving it to a file (e.g., 'app.py') and running it with 'python app.py'. This will start the Flask development server, which will serve up 'index.html' when you navigate to 'http://localhost:5000/' in your web browser.\n",
    "\"\"\"\n",
    "\n",
    "updated_string = string.replace(\"'''\", \"```\")\n",
    "print(updated_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
