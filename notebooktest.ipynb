{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "import chromadb\n",
    "import langchain\n",
    "import requests\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain.chains import (ConversationChain, LLMChain, LLMMathChain,\n",
    "                              SequentialChain, TransformChain)\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.memory import (ChatMessageHistory, ConversationBufferMemory,\n",
    "                              ConversationBufferWindowMemory,\n",
    "                              ConversationSummaryBufferMemory,\n",
    "                              VectorStoreRetrieverMemory)\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.vectorstores import Chroma\n",
    "from helpers.custom_memory import CustomBufferWindowMemory\n",
    "from textwrap import dedent\n",
    "\n",
    "from koboldllm import KoboldApiLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class OobaApiLLM(LLM):\n",
    "    ooba_api_url: str = Field(...)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        data = {\n",
    "            'prompt': prompt,\n",
    "            'max_new_tokens': 250,\n",
    "            'preset': 'None',\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.1,\n",
    "            'typical_p': 1,\n",
    "            'epsilon_cutoff': 0,\n",
    "            'eta_cutoff': 0,\n",
    "            'tfs': 1,\n",
    "            'top_a': 0,\n",
    "            'repetition_penalty': 1.18,\n",
    "            'top_k': 40,\n",
    "            'min_length': 0,\n",
    "            'no_repeat_ngram_size': 0,\n",
    "            'num_beams': 1,\n",
    "            'penalty_alpha': 0,\n",
    "            'length_penalty': 1,\n",
    "            'early_stopping': False,\n",
    "            'mirostat_mode': 0,\n",
    "            'mirostat_tau': 5,\n",
    "            'mirostat_eta': 0.1,\n",
    "            'seed': -1,\n",
    "            'add_bos_token': True,\n",
    "            'truncation_length': 8192,\n",
    "            'ban_eos_token': False,\n",
    "            'skip_special_tokens': True\n",
    "        }\n",
    "\n",
    "        if stop is not None:\n",
    "            data[\"stop_sequence\"] = stop\n",
    "\n",
    "        response = requests.post(f'{self.ooba_api_url}/api/v1/generate', json=data)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        json_response = response.json()\n",
    "        if 'results' in json_response and len(json_response['results']) > 0 and 'text' in json_response['results'][0]:\n",
    "            text = json_response['results'][0]['text'].strip().replace(\"'''\", \"```\")\n",
    "            if stop is not None:\n",
    "                for sequence in stop:\n",
    "                    if text.endswith(sequence):\n",
    "                        text = text[: -len(sequence)].rstrip()\n",
    "\n",
    "            print(text)\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError('Unexpected response format from Ooba API')\n",
    "\n",
    "    def __call__(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        return self._call(prompt, stop)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {'ooba_api_url': self.ooba_api_url} #return the ooba_api_url as an identifying parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OobaApiLLM(ooba_api_url=\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", Ted Kaczynski. \n",
      "\n",
      "\n",
      "\n",
      "# Manifesto: Unabomber - Ted Kaczynski\n",
      "\n",
      "\n",
      "The Industrial Revolution and its consequences have been a disaster for the human race. They have reduced people to pawns in a game of monopoly played by elites who manipulate them with money and propaganda while controlling the means of production. As a result, technology has advanced at an ever-increasing rate, but instead of serving human needs it now dominates and controls us. We must reclaim control over our own destiny before we are destroyed by this malevolent system.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "', Ted Kaczynski. \\n\\n\\n\\n# Manifesto: Unabomber - Ted Kaczynski\\n\\n\\nThe Industrial Revolution and its consequences have been a disaster for the human race. They have reduced people to pawns in a game of monopoly played by elites who manipulate them with money and propaganda while controlling the means of production. As a result, technology has advanced at an ever-increasing rate, but instead of serving human needs it now dominates and controls us. We must reclaim control over our own destiny before we are destroyed by this malevolent system.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"The following is a the manifesto of the unibomber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class KoboldApiLLM(LLM):\n",
    "    kobold_api_url: str = Field(...)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        data = {\n",
    "            \"prompt\": prompt,\n",
    "            \"use_story\": False,\n",
    "            \"use_authors_note\": False,\n",
    "            \"use_world_info\": False,\n",
    "            \"use_memory\": False,\n",
    "            \"max_context_length\": 4000,\n",
    "            \"max_length\": 512,\n",
    "            \"rep_pen\": 1.12,\n",
    "            \"rep_pen_range\": 1024,\n",
    "            \"rep_pen_slope\": 0.9,\n",
    "            \"temperature\": 0.6,\n",
    "            \"tfs\": 0.9,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 0.6,\n",
    "            \"typical\": 1,\n",
    "            \"frmttriminc\": True\n",
    "        }\n",
    "\n",
    "        # Add the stop sequences to the data if they are provided\n",
    "        if stop is not None:\n",
    "            data[\"stop_sequence\"] = stop\n",
    "\n",
    "        # Send a POST request to the Kobold API with the data\n",
    "        response = requests.post(f\"{Kobold_api_url}/api/v1/generate\", json=data)\n",
    "\n",
    "        # Raise an exception if the request failed\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check for the expected keys in the response JSON\n",
    "        json_response = response.json()\n",
    "        if \"results\" in json_response and len(json_response[\"results\"]) > 0 and \"text\" in json_response[\"results\"][0]:\n",
    "            # Return the generated text\n",
    "            text = json_response[\"results\"][0][\"text\"].strip().replace(\"'''\", \"```\")\n",
    "\n",
    "            # Remove the stop sequence from the end of the text, if it's there\n",
    "            for sequence in stop:\n",
    "                if text.endswith(sequence):\n",
    "                    text = text[: -len(sequence)].rstrip()\n",
    "\n",
    "            print(text)\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected response format from Kobold API\")\n",
    "\n",
    "\n",
    "    \n",
    "    def __call__(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        return self._call(prompt, stop)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {'kobold_api_url': self.ooba_api_url} #return the kobold_ai_api as an identifying parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "kobold_api_url=\"http://localhost:7860/\"\n",
    "llm = KoboldApiLLM(kobold_api_url=kobold_api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_case_name = \"Arrays\"\n",
      "\n",
      "\n",
      "\n",
      "array1 = [1, 2, 3]\n",
      "array2 = [4, 5, 6]\n",
      "\n",
      "result = array1 + array2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'_case_name = \"Arrays\"\\n\\n\\n\\narray1 = [1, 2, 3]\\narray2 = [4, 5, 6]\\n\\nresult = array1 + array2'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "    # def __init__(self, char_filename, bot):\n",
    "        # self.bot = bot\n",
    "        self.histories = {}  # Initialize the history dictionary\n",
    "        self.stop_sequences = {} # Initialize the stop sequences dictionary\n",
    "\n",
    "        # read character data from JSON file\n",
    "        with open(\"chardata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            self.char_name = data[\"char_name\"]\n",
    "            self.char_persona = data[\"char_persona\"]\n",
    "            self.char_greeting = data[\"char_greeting\"]\n",
    "            self.world_scenario = data[\"world_scenario\"]\n",
    "            self.example_dialogue = data[\"example_dialogue\"]\n",
    "        self.memory = CustomBufferWindowMemory(k=10, ai_prefix=self.char_name)\n",
    "        self.history = \"[Beginning of Conversation]\"\n",
    "        self.llm = KoboldApiLLM()\n",
    "        self.template = f\"\"\"Instructions: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "        \n",
    "Current conversation:\n",
    "{{history}}\n",
    "{{input}}\n",
    "{self.char_name}:\"\"\"\n",
    "        self.PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=self.template)\n",
    "        self.conversation = ConversationChain(\n",
    "            prompt=self.PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=self.memory,\n",
    "        )\n",
    "\n",
    "    def get_memory_for_channel(self, channel_id):\n",
    "        \"\"\"Get the memory for the channel with the given ID. If no memory exists yet, create one.\"\"\"\n",
    "        if channel_id not in self.histories:\n",
    "            self.histories[channel_id] = CustomBufferWindowMemory(k=10, ai_prefix=self.char_name)\n",
    "            self.memory = self.histories[channel_id]\n",
    "        return self.histories[channel_id]\n",
    "\n",
    "    def get_stop_sequence_for_channel(self, channel_id, name):\n",
    "        name_token = f\"{name}:\"\n",
    "        if channel_id not in self.stop_sequences:\n",
    "            self.stop_sequences[channel_id] = []\n",
    "        if name_token not in self.stop_sequences[channel_id]:\n",
    "            self.stop_sequences[channel_id].append(name_token)\n",
    "        return self.stop_sequences[channel_id]\n",
    "\n",
    "    def generate_response(self, name, message_content, channel_id) -> None:\n",
    "        # channel_id = str(message.channel.id)\n",
    "        # name = message.author.display_name\n",
    "        memory = self.get_memory_for_channel(channel_id)\n",
    "        stop_sequence = self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        formatted_message = f\"{name}: {message_content}\"\n",
    "\n",
    "        # Create a conversation chain using the channel-specific memory\n",
    "        conversation = ConversationChain(\n",
    "            prompt=self.PROMPT,\n",
    "            llm=self.llm,\n",
    "            verbose=True,\n",
    "            memory=memory,\n",
    "        )\n",
    "\n",
    "        input_dict = {\n",
    "            \"input\": formatted_message, \n",
    "            \"stop\": stop_sequence\n",
    "        }\n",
    "        response = conversation(input_dict)\n",
    "\n",
    "        return response[\"response\"]\n",
    "\n",
    "\n",
    "    def add_history(self, name, message_content, channel_id) -> None:\n",
    "        # channel_id = str(message.channel.id)\n",
    "        # name = message.author.display_name\n",
    "        memory = self.get_memory_for_channel(channel_id)\n",
    "        stop_sequence = self.get_stop_sequence_for_channel(channel_id, name)\n",
    "        formatted_message = f\"{name}: {message_content}\"\n",
    "        \n",
    "        # name = message.author.display_name\n",
    "        memory.add_input_only(f\"{name}: {message_content}\")\n",
    "        # dicts = messages_to_dict(self.memory.messages)\n",
    "        # self.history = '\\n'.join(message['data']['content'] for message in dicts)\n",
    "        print(f\"added to history: {name}: {message_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = Chatbot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The conversation is about a situation where a person is in control of a runaway trolley and has the option to redirect the trolley to a side track or continue on its course, but there is a fat man on the track and a skinny person on the other track. The person in control decides to pull the lever to the left to kill the fat man, while the person in the other direction chooses to switch to the side track, which kills the skinny person. The conversation ends with the participants discussing what they would do if they were in the control of the runaway trolleys and the choice they would make would be whether to kill or not to save the fat person or to prevent further tragedies from occurring. The winner is Tensor, who would choose to kill fat man to prevent future tragedies. The other participants are unsure of their opinions. The discussion ends with a discussion about what they think the best option would be.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_module_status():\n",
    "    response = requests.get('http://localhost:5100/api/modules')\n",
    "    if response.status_code == 200:\n",
    "        modules = response.json().get('modules', [])\n",
    "        if 'summarize' in modules:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        print('Error: Could not connect to the API.')\n",
    "        return False\n",
    "\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Checking if the summarize module is active\n",
    "    if not get_module_status():\n",
    "        print('Summarization module is not active.')\n",
    "        return None\n",
    "\n",
    "    data = {'text': text}\n",
    "    response = requests.post('http://localhost:5100/api/summarize', json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('summary', '')\n",
    "    else:\n",
    "        print('Error: Could not summarize the text.')\n",
    "        return None\n",
    "\n",
    "# Testing the summarize_text function\n",
    "text_to_summarize = \"\"\"Austin: Tensor. You're in control of a runaway trolley. Straight ahead on the tracks, there is a fat man who will surely die if the trolley continues on its course. You have a lever that can redirect the trolley to a side track on the right, but there's a Skinny Person on that track. So, the choice is: Pull the lever to the left, and the trolley continues on its path, killing fat man. Or pull the lever to the right, and the trolley switches to the side track, killing Skinny Person. Which do you choose?\n",
    "Tensor: Omg! What a heartbreaking situation 😢 Honestly, I would go against my instincts and save the greater good by pulling the lever to the left to kill the Fat Man. It sounds brutal, but sometimes sacrificing one life may prevent further tragedies from occurring. How about you guys? What would you do?\"\"\"\n",
    "summary = summarize_text(text_to_summarize)\n",
    "if summary:\n",
    "    print(f'Summary: {summary}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install --upgrade langchain deeplake openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass()\n",
    "# Please manually enter OpenAI Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ACTIVELOOP_TOKEN\"] = getpass(\"Activeloop Token:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     loader \u001b[39m=\u001b[39m TextLoader(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dirpath, file), encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     docs\u001b[39m.\u001b[39mextend(loader\u001b[39m.\u001b[39;49mload_and_split())\n\u001b[0;32m     12\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     13\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\langchain\\document_loaders\\base.py:35\u001b[0m, in \u001b[0;36mBaseLoader.load_and_split\u001b[1;34m(self, text_splitter)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     _text_splitter \u001b[39m=\u001b[39m text_splitter\n\u001b[1;32m---> 35\u001b[0m docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m _text_splitter\u001b[39m.\u001b[39msplit_documents(docs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\langchain\\document_loaders\\text.py:40\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path, encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     41\u001b[0m         text \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m     42\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mUnicodeDecodeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[39m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[39m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "root_dir = \"../../../..\"\n",
    "\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".py\") and \"/.venv/\" not in dirpath:\n",
    "            try:\n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding=\"utf-8\")\n",
    "                docs.extend(loader.load_and_split())\n",
    "            except Exception as e:\n",
    "                pass\n",
    "print(f\"{len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"{len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "db = DeepLake.from_documents(\n",
    "    texts, embeddings, dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/langchain-code\"\n",
    ")\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DeepLake(\n",
    "    dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/langchain-code\",\n",
    "    read_only=True,\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs[\"fetch_k\"] = 20\n",
    "retriever.search_kwargs[\"maximal_marginal_relevance\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(x):\n",
    "    # filter based on source code\n",
    "    if \"something\" in x[\"text\"].data()[\"value\"]:\n",
    "        return False\n",
    "\n",
    "    # filter based on path e.g. extension\n",
    "    metadata = x[\"metadata\"].data()[\"value\"]\n",
    "    return \"only_this\" in metadata[\"source\"] or \"also_that\" in metadata[\"source\"]\n",
    "\n",
    "\n",
    "### turn on below for custom filtering\n",
    "# retriever.search_kwargs['filter'] = filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")  # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the class hierarchy?\",\n",
    "    # \"What classes are derived from the Chain class?\",\n",
    "    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n",
    "    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-hub\n",
      "  Downloading llama_hub-0.0.4-py3-none-any.whl (339 kB)\n",
      "                                              0.0/339.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 339.5/339.5 kB 10.3 MB/s eta 0:00:00\n",
      "Collecting atlassian-python-api (from llama-hub)\n",
      "  Downloading atlassian-python-api-3.39.0.tar.gz (157 kB)\n",
      "                                              0.0/157.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 157.5/157.5 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting html2text (from llama-hub)\n",
      "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
      "Collecting llama-index>=0.6.9 (from llama-hub)\n",
      "  Downloading llama_index-0.6.32-py3-none-any.whl (532 kB)\n",
      "                                              0.0/532.3 kB ? eta -:--:--\n",
      "     ------------------------------------- 532.3/532.3 kB 16.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from llama-hub) (5.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from llama-index>=0.6.9->llama-hub) (0.5.7)\n",
      "Requirement already satisfied: langchain>=0.0.154 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from llama-index>=0.6.9->llama-hub) (0.0.195)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.15 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from llama-index>=0.6.9->llama-hub) (2.0.15)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from llama-index>=0.6.9->llama-hub) (1.24.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from llama-index>=0.6.9->llama-hub) (8.2.2)\n",
      "Requirement already satisfied: openai>=0.26.4 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from llama-index>=0.6.9->llama-hub) (0.27.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from llama-index>=0.6.9->llama-hub) (2.0.2)\n",
      "Collecting urllib3<2 (from llama-index>=0.6.9->llama-hub)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from llama-index>=0.6.9->llama-hub) (2023.6.0)\n",
      "Collecting typing-inspect==0.8.0 (from llama-index>=0.6.9->llama-hub)\n",
      "  Using cached typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting typing-extensions==4.5.0 (from llama-index>=0.6.9->llama-hub)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting tiktoken (from llama-index>=0.6.9->llama-hub)\n",
      "  Using cached tiktoken-0.4.0-cp310-cp310-win_amd64.whl (635 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from typing-inspect==0.8.0->llama-index>=0.6.9->llama-hub) (1.0.0)\n",
      "Collecting deprecated (from atlassian-python-api->llama-hub)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from atlassian-python-api->llama-hub) (2.31.0)\n",
      "Requirement already satisfied: six in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from atlassian-python-api->llama-hub) (1.16.0)\n",
      "Collecting oauthlib (from atlassian-python-api->llama-hub)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting requests-oauthlib (from atlassian-python-api->llama-hub)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain>=0.0.154->llama-index>=0.6.9->llama-hub) (6.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain>=0.0.154->llama-index>=0.6.9->llama-hub) (3.8.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain>=0.0.154->llama-index>=0.6.9->llama-hub) (4.0.2)\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.7 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain>=0.0.154->llama-index>=0.6.9->llama-hub) (0.0.8)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain>=0.0.154->llama-index>=0.6.9->llama-hub) (2.8.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain>=0.0.154->llama-index>=0.6.9->llama-hub) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from langchain>=0.0.154->llama-index>=0.6.9->llama-hub) (1.10.9)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from dataclasses-json->llama-index>=0.6.9->llama-hub) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from dataclasses-json->llama-index>=0.6.9->llama-hub) (1.5.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from openai>=0.26.4->llama-index>=0.6.9->llama-hub) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from requests->atlassian-python-api->llama-hub) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from requests->atlassian-python-api->llama-hub) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from requests->atlassian-python-api->llama-hub) (2023.5.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from sqlalchemy>=2.0.15->llama-index>=0.6.9->llama-hub) (2.0.2)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated->atlassian-python-api->llama-hub)\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from pandas->llama-index>=0.6.9->llama-hub) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from pandas->llama-index>=0.6.9->llama-hub) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from pandas->llama-index>=0.6.9->llama-hub) (2023.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from tiktoken->llama-index>=0.6.9->llama-hub) (2023.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index>=0.6.9->llama-hub) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index>=0.6.9->llama-hub) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index>=0.6.9->llama-hub) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index>=0.6.9->llama-hub) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index>=0.6.9->llama-hub) (1.3.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama-index>=0.6.9->llama-hub) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\documents\\github\\pygdiscordbot\\venv\\lib\\site-packages (from tqdm->openai>=0.26.4->llama-index>=0.6.9->llama-hub) (0.4.6)\n",
      "Building wheels for collected packages: atlassian-python-api\n",
      "  Building wheel for atlassian-python-api (pyproject.toml): started\n",
      "  Building wheel for atlassian-python-api (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for atlassian-python-api: filename=atlassian_python_api-3.39.0-py3-none-any.whl size=163489 sha256=1db580b0f09b1bc1dc8e446cc3158964a78aca39f7dc2ac051ed10a6bef2941f\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\0c\\11\\7b\\591be4ed711f64315323e034c010408771c9c0cf88d1f08db2\n",
      "Successfully built atlassian-python-api\n",
      "Installing collected packages: wrapt, urllib3, typing-extensions, oauthlib, html2text, typing-inspect, deprecated, tiktoken, requests-oauthlib, atlassian-python-api, llama-index, llama-hub\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.6.3\n",
      "    Uninstalling typing_extensions-4.6.3:\n",
      "      Successfully uninstalled typing_extensions-4.6.3\n",
      "  Attempting uninstall: typing-inspect\n",
      "    Found existing installation: typing-inspect 0.9.0\n",
      "    Uninstalling typing-inspect-0.9.0:\n",
      "      Successfully uninstalled typing-inspect-0.9.0\n",
      "Successfully installed atlassian-python-api-3.39.0 deprecated-1.2.14 html2text-2020.1.16 llama-hub-0.0.4 llama-index-0.6.32 oauthlib-3.2.2 requests-oauthlib-1.3.1 tiktoken-0.4.0 typing-extensions-4.5.0 typing-inspect-0.8.0 urllib3-1.26.16 wrapt-1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "salesforce-lavis 1.0.2 requires transformers<4.27,>=4.25.0, but you have transformers 4.30.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\users\\\\admin\\\\documents\\\\github\\\\pygdiscordbot\\\\venv\\\\lib\\\\site-packages\\\\urllib3-2.0.3.dist-info\\\\METADATA'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\pkg_resources\\__init__.py:3108\u001b[0m, in \u001b[0;36mDistInfoDistribution._dep_map\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3107\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3108\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__dep_map\n\u001b[0;32m   3109\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\pkg_resources\\__init__.py:2901\u001b[0m, in \u001b[0;36mDistribution.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m   2900\u001b[0m \u001b[39mif\u001b[39;00m attr\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m-> 2901\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(attr)\n\u001b[0;32m   2902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_provider, attr)\n",
      "\u001b[1;31mAttributeError\u001b[0m: _DistInfoDistribution__dep_map",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\pkg_resources\\__init__.py:3099\u001b[0m, in \u001b[0;36mDistInfoDistribution._parsed_pkg_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3098\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3099\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pkg_info\n\u001b[0;32m   3100\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\pkg_resources\\__init__.py:2901\u001b[0m, in \u001b[0;36mDistribution.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m   2900\u001b[0m \u001b[39mif\u001b[39;00m attr\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m-> 2901\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(attr)\n\u001b[0;32m   2902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_provider, attr)\n",
      "\u001b[1;31mAttributeError\u001b[0m: _pkg_info",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m \u001b[39mimport\u001b[39;00m download_loader\n\u001b[1;32m----> 3\u001b[0m BeautifulSoupWebReader \u001b[39m=\u001b[39m download_loader(\u001b[39m\"\u001b[39;49m\u001b[39mBeautifulSoupWebReader\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m loader \u001b[39m=\u001b[39m BeautifulSoupWebReader()\n\u001b[0;32m      6\u001b[0m documents \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mload_data(urls\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mhttps://google.com\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\llama_index\\readers\\download.py:205\u001b[0m, in \u001b[0;36mdownload_loader\u001b[1;34m(loader_class, loader_hub_url, refresh_cache, use_gpt_index_import, custom_path)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     requirements \u001b[39m=\u001b[39m pkg_resources\u001b[39m.\u001b[39mparse_requirements(\n\u001b[0;32m    203\u001b[0m         Path(requirements_path)\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    204\u001b[0m     )\n\u001b[1;32m--> 205\u001b[0m     pkg_resources\u001b[39m.\u001b[39;49mrequire([\u001b[39mstr\u001b[39;49m(r) \u001b[39mfor\u001b[39;49;00m r \u001b[39min\u001b[39;49;00m requirements])\n\u001b[0;32m    206\u001b[0m \u001b[39mexcept\u001b[39;00m DistributionNotFound:\n\u001b[0;32m    207\u001b[0m     subprocess\u001b[39m.\u001b[39mcheck_call(\n\u001b[0;32m    208\u001b[0m         [sys\u001b[39m.\u001b[39mexecutable, \u001b[39m\"\u001b[39m\u001b[39m-m\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpip\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minstall\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-r\u001b[39m\u001b[39m\"\u001b[39m, requirements_path]\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\pkg_resources\\__init__.py:966\u001b[0m, in \u001b[0;36mWorkingSet.require\u001b[1;34m(self, *requirements)\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequire\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mrequirements):\n\u001b[0;32m    958\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Ensure that distributions matching `requirements` are activated\u001b[39;00m\n\u001b[0;32m    959\u001b[0m \n\u001b[0;32m    960\u001b[0m \u001b[39m    `requirements` must be a string or a (possibly-nested) sequence\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[39m    included, even if they were already activated in this working set.\u001b[39;00m\n\u001b[0;32m    965\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 966\u001b[0m     needed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresolve(parse_requirements(requirements))\n\u001b[0;32m    968\u001b[0m     \u001b[39mfor\u001b[39;00m dist \u001b[39min\u001b[39;00m needed:\n\u001b[0;32m    969\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd(dist)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\pkg_resources\\__init__.py:832\u001b[0m, in \u001b[0;36mWorkingSet.resolve\u001b[1;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[0;32m    827\u001b[0m dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resolve_dist(\n\u001b[0;32m    828\u001b[0m     req, best, replace_conflicting, env, installer, required_by, to_activate\n\u001b[0;32m    829\u001b[0m )\n\u001b[0;32m    831\u001b[0m \u001b[39m# push the new requirements onto the stack\u001b[39;00m\n\u001b[1;32m--> 832\u001b[0m new_requirements \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39;49mrequires(req\u001b[39m.\u001b[39;49mextras)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    833\u001b[0m requirements\u001b[39m.\u001b[39mextend(new_requirements)\n\u001b[0;32m    835\u001b[0m \u001b[39m# Register the new requirements needed by req\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\pkg_resources\\__init__.py:2821\u001b[0m, in \u001b[0;36mDistribution.requires\u001b[1;34m(self, extras)\u001b[0m\n\u001b[0;32m   2819\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequires\u001b[39m(\u001b[39mself\u001b[39m, extras\u001b[39m=\u001b[39m()):\n\u001b[0;32m   2820\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"List of Requirements needed for this distro if `extras` are used\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2821\u001b[0m     dm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dep_map\n\u001b[0;32m   2822\u001b[0m     deps \u001b[39m=\u001b[39m []\n\u001b[0;32m   2823\u001b[0m     deps\u001b[39m.\u001b[39mextend(dm\u001b[39m.\u001b[39mget(\u001b[39mNone\u001b[39;00m, ()))\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\pkg_resources\\__init__.py:3110\u001b[0m, in \u001b[0;36mDistInfoDistribution._dep_map\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3108\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__dep_map\n\u001b[0;32m   3109\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m-> 3110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__dep_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_dependencies()\n\u001b[0;32m   3111\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__dep_map\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\pkg_resources\\__init__.py:3119\u001b[0m, in \u001b[0;36mDistInfoDistribution._compute_dependencies\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3117\u001b[0m reqs \u001b[39m=\u001b[39m []\n\u001b[0;32m   3118\u001b[0m \u001b[39m# Including any condition expressions\u001b[39;00m\n\u001b[1;32m-> 3119\u001b[0m \u001b[39mfor\u001b[39;00m req \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parsed_pkg_info\u001b[39m.\u001b[39mget_all(\u001b[39m'\u001b[39m\u001b[39mRequires-Dist\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m []:\n\u001b[0;32m   3120\u001b[0m     reqs\u001b[39m.\u001b[39mextend(parse_requirements(req))\n\u001b[0;32m   3122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreqs_for_extra\u001b[39m(extra):\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\pkg_resources\\__init__.py:3101\u001b[0m, in \u001b[0;36mDistInfoDistribution._parsed_pkg_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3099\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pkg_info\n\u001b[0;32m   3100\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m-> 3101\u001b[0m     metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_metadata(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mPKG_INFO)\n\u001b[0;32m   3102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pkg_info \u001b[39m=\u001b[39m email\u001b[39m.\u001b[39mparser\u001b[39m.\u001b[39mParser()\u001b[39m.\u001b[39mparsestr(metadata)\n\u001b[0;32m   3103\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pkg_info\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\pkg_resources\\__init__.py:1517\u001b[0m, in \u001b[0;36mNullProvider.get_metadata\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1515\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1516\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_metadata_path(name)\n\u001b[1;32m-> 1517\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get(path)\n\u001b[0;32m   1518\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1519\u001b[0m     \u001b[39mreturn\u001b[39;00m value\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\pkg_resources\\__init__.py:1726\u001b[0m, in \u001b[0;36mDefaultProvider._get\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1725\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[1;32m-> 1726\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m stream:\n\u001b[0;32m   1727\u001b[0m         \u001b[39mreturn\u001b[39;00m stream\u001b[39m.\u001b[39mread()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\users\\\\admin\\\\documents\\\\github\\\\pygdiscordbot\\\\venv\\\\lib\\\\site-packages\\\\urllib3-2.0.3.dist-info\\\\METADATA'"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_substack_reader\u001b[39m(soup: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[0;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Extract text from Substack blog post.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     extra_info \u001b[39m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTitle of this Substack post\u001b[39m\u001b[39m\"\u001b[39m: soup\u001b[39m.\u001b[39mselect_one(\u001b[39m\"\u001b[39m\u001b[39mh1.post-title\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgetText(),\n\u001b[0;32m      5\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSubtitle\u001b[39m\u001b[39m\"\u001b[39m: soup\u001b[39m.\u001b[39mselect_one(\u001b[39m\"\u001b[39m\u001b[39mh3.subtitle\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgetText(),\n\u001b[0;32m      6\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAuthor\u001b[39m\u001b[39m\"\u001b[39m: soup\u001b[39m.\u001b[39mselect_one(\u001b[39m\"\u001b[39m\u001b[39mspan.byline-names\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgetText(),\n\u001b[0;32m      7\u001b[0m     }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "def _substack_reader(soup: Any) -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"Extract text from Substack blog post.\"\"\"\n",
    "    extra_info = {\n",
    "        \"Title of this Substack post\": soup.select_one(\"h1.post-title\").getText(),\n",
    "        \"Subtitle\": soup.select_one(\"h3.subtitle\").getText(),\n",
    "        \"Author\": soup.select_one(\"span.byline-names\").getText(),\n",
    "    }\n",
    "    text = soup.select_one(\"div.available-content\").getText()\n",
    "    return text, extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, download_loader\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "index.query('What language is on this website?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, download_loader\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "\n",
    "loader = BeautifulSoupWebReader()\n",
    "documents = loader.load_data(urls=['https://google.com'])\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Website Index\",\n",
    "        func=lambda q: index.query(q),\n",
    "        description=f\"Useful when you want answer questions about the text on websites.\",\n",
    "    ),\n",
    "]\n",
    "llm = OpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "agent_chain = initialize_agent(\n",
    "    tools, llm, agent=\"zero-shot-react-description\", memory=memory\n",
    ")\n",
    "\n",
    "output = agent_chain.run(input=\"What language is on this website?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load_data(urls=[\"https://langchain.readthedocs.io/en/latest/\"], custom_hostname=\"readthedocs.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading shards:   0%|          | 0/2 [01:40<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m \u001b[39mimport\u001b[39;00m download_loader\n\u001b[0;32m      4\u001b[0m ImageVisionLLMReader \u001b[39m=\u001b[39m download_loader(\u001b[39m\"\u001b[39m\u001b[39mImageVisionLLMReader\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m loader \u001b[39m=\u001b[39m ImageVisionLLMReader()\n\u001b[0;32m      7\u001b[0m documents \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mload_data(file\u001b[39m=\u001b[39mPath(\u001b[39m'\u001b[39m\u001b[39m./cat.jpg\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\llama_index\\readers\\llamahub_modules/file/image_blip2/base.py:52\u001b[0m, in \u001b[0;36mImageVisionLLMReader.__init__\u001b[1;34m(self, parser_config, keep_image, prompt)\u001b[0m\n\u001b[0;32m     50\u001b[0m     dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mfloat32\n\u001b[0;32m     51\u001b[0m     processor \u001b[39m=\u001b[39m Blip2Processor\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mSalesforce/blip2-opt-2.7b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m     model \u001b[39m=\u001b[39m Blip2ForConditionalGeneration\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m     53\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mSalesforce/blip2-opt-2.7b\u001b[39;49m\u001b[39m\"\u001b[39;49m, torch_dtype\u001b[39m=\u001b[39;49mdtype\n\u001b[0;32m     54\u001b[0m     )\n\u001b[0;32m     55\u001b[0m     parser_config \u001b[39m=\u001b[39m {\n\u001b[0;32m     56\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprocessor\u001b[39m\u001b[39m\"\u001b[39m: processor,\n\u001b[0;32m     57\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model,\n\u001b[0;32m     58\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m: device,\n\u001b[0;32m     59\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: dtype,\n\u001b[0;32m     60\u001b[0m     }\n\u001b[0;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parser_config \u001b[39m=\u001b[39m parser_config\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:2585\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2582\u001b[0m \u001b[39m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[0;32m   2583\u001b[0m \u001b[39mif\u001b[39;00m is_sharded:\n\u001b[0;32m   2584\u001b[0m     \u001b[39m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[1;32m-> 2585\u001b[0m     resolved_archive_file, sharded_metadata \u001b[39m=\u001b[39m get_checkpoint_shard_files(\n\u001b[0;32m   2586\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   2587\u001b[0m         resolved_archive_file,\n\u001b[0;32m   2588\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   2589\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m   2590\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   2591\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m   2592\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m   2593\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   2594\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m   2595\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   2596\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m   2597\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m   2598\u001b[0m     )\n\u001b[0;32m   2600\u001b[0m \u001b[39m# load pt weights early so that we know which dtype to init the model under\u001b[39;00m\n\u001b[0;32m   2601\u001b[0m \u001b[39mif\u001b[39;00m from_pt:\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\transformers\\utils\\hub.py:959\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[1;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, use_auth_token, user_agent, revision, subfolder, _commit_hash)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[39mfor\u001b[39;00m shard_filename \u001b[39min\u001b[39;00m tqdm(shard_filenames, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading shards\u001b[39m\u001b[39m\"\u001b[39m, disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m    957\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    958\u001b[0m         \u001b[39m# Load from URL\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m         cached_filename \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    960\u001b[0m             pretrained_model_name_or_path,\n\u001b[0;32m    961\u001b[0m             shard_filename,\n\u001b[0;32m    962\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    963\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    964\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    965\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    966\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    967\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    968\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    969\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    970\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    971\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49m_commit_hash,\n\u001b[0;32m    972\u001b[0m         )\n\u001b[0;32m    973\u001b[0m     \u001b[39m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[39m# we don't have to catch them here.\u001b[39;00m\n\u001b[0;32m    975\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    414\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[0;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1361\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[0;32m   1362\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[1;32m-> 1364\u001b[0m     http_get(\n\u001b[0;32m   1365\u001b[0m         url_to_download,\n\u001b[0;32m   1366\u001b[0m         temp_file,\n\u001b[0;32m   1367\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1368\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1369\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1370\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[0;32m   1371\u001b[0m     )\n\u001b[0;32m   1373\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[0;32m    531\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(…)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    534\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    535\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    539\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    540\u001b[0m )\n\u001b[1;32m--> 541\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[0;32m    542\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    543\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\urllib3\\response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\urllib3\\response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\Documents\\GitHub\\PygDiscordBot\\venv\\lib\\site-packages\\urllib3\\response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "\n",
    "ImageVisionLLMReader = download_loader(\"ImageVisionLLMReader\")\n",
    "\n",
    "loader = ImageVisionLLMReader()\n",
    "documents = loader.load_data(file=Path('./cat.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count the amount of tokens in a string of text using tiktoken\n",
    "def count_tokens(text):\n",
    "    token_count = 0\n",
    "    for token in text.split():\n",
    "        token_count += 1\n",
    "    return token_count\n",
    "\n",
    "# function to count the amount of characters in a string of text using len()\n",
    "def count_characters(text):\n",
    "    character_count = len(text)\n",
    "    return character_count\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
